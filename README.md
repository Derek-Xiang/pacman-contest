# pacman-contest <br>
This pacman robot is complemented with two strategy: <br>
Challenges and Improvements
In this Pacman tournament, most likely other teams are using two separate Pacman agents with two different approaches to implementing their Pacman agents. First, we challenge ourselves which we decided to implement only one main Pacman agent with a mix of multiple techniques. As the game goes on, we establish a system to switch the "job" they suppose to do on the fly. Second, handling the characters exchange using the concept of the Minimax Algorithm is extremely hard to achieve. Not only we are using only one agent class as the guide of the Pacman with the mix techniques, but also we tried to maximize the score, travel distance, and survive. In the first place, we try to implement Monte Carlo Tree Search (MCT). However, the result of the original MCT implementation plays like a headless fly which is extremely useless. Furthermore, the computing time for MCT is too long to limit to 1s, and the randomness does not help at all. As a result, we decide to use UCT which yield a much better solution. The process that tries to use the Minimax concept is also very challenging, and we took advantage of the process time to learn the map. The last but not least, we also use the A* Algorithm as an online approach where each time will give action and followed by the new state after each action.

Techniques
In this section, we will discuss how we implement different techniques that used in Pacman agent.

A* Heuristic Search
A* Algorithm is a classic offline heuristic search algorithm. Since we are playing this game as a fixed individual situation for every single step, we decide that we are going to trade A* as an online approach where each action is chosen by the first action of the plan generated from A*. Before generating the action for the next move, we update a new state based on the previous action. In addition, the heuristic function that used to estimate the heuristic value is Manhattan Distance. Manhattan distance is the best estimate value for our case since we need a relatively fast in computing and make sense when we calculate a heuristic value of a maze problem. The goal of the A* is to choose the best action for a given target and try to avoid to be eaten by the opponents.

Minimax Algorithm Decision Theory
The whole Pacman agent is based on the concept of Minimax. Minimax is the oldest artificial intelligence theory which is a decision rule based theory applied to decision theory, game theory, statistics and philosophy for maximizing the minimum gain and minimizing the maximum loss. Our Pacman agent is mainly based on this theory to design and implement. Based on our experiments with this theory, we implement couple highlight features that used in our programs such as the concept of safe food, the greedy approach of A*, switching character of agent, separate agents targets, and the position of defending. In experiments & Features section we will demonstrate the concepts in details.

Upper Confidence Bound applied to Trees (UCT) on Monte Carlo Tree Search
The UCT is an AI algorithm which builds a tree-like data structure and randomly explores the game world and get rewards, then send it back to the root nodes to help making decisions, which will be choosed as the next reasonable move. For this game, we use a dictionary as a container to hold the position as keys and a series of information as the value for each key. The value includes its position, its parent position, total reward, number of visits and a list of its child nodes. Also, in order to prevent backpropagating predicament, which going backwards makes child nodes as parent nodes, we used another two lists to store the path, and the index of each step. By having those, we can track the sequence of steps and do backpropagation without crash, however, which consumes more space and computation. Moreover, we give the safe food(which will be introduce latter) a large positive score 150 to encourage the pacman to eat, and -1 for the spot that has been visited so that pacman tends not to go backwards. The total rewards together the number of visits and the number of using this action, we can calculate the UCB to convert the MCT to UCT, which should have a better balance between exploitation and exploration. However, with progressing, under the 1 minute constraints, it can be found that UCT performs not as good as what we want, and also, sometimes, with unknown problem of the server, it performs extremely poor, so we put more exffort on other methods but we still put the code on the same folder, the detail of running is in readme in wiki.

Experiments & Features
In this section, we will explain different experiments and features that made to the final version of Pacman agent.

SafeFood
The best feature in this project is the concept of safe food which the Pacman most likely can safely eat all safe food without getting caught in an alleyway. The safe food utilizes the pregame process time to learn the map which gives us a list of coordinate with the cost to another coordinate which is a chowk point of a dead-end. For each state, we calculate both agents safe food using the list we mentioned above. Two lists of safe food will use to choose the action of each Pacman agents. Using this experimental feature, we are able to achieve the Minimax theory that minimizes the death and maximize the rewards(foods). The safe food is calculated using the list we generated from map learning process and alter the distance between myself and food plus return trip as well as the distance between the nearest ghost and the chowk point. The cost in the list will able to help Pacman know whether is safe to eat the food or not. It was the best feeling in the world when a ghost tried to chase my agent down, and we ate all the food that is safe along the way with the constant distance of one.

Utilize safe food and greedy A* search

Greedy A* Heuristic Search
Our A* heuristic search using a greedy approach to calculate the shortest and safest action relative to current state whether going for food or find the safe path back to the home base. With the safe food concept, it can safely eat as much food as possible. However, it is not an optimal solution which may cause the death of the Pacman when food is in a deep dead-end and Pacman don't know the position of its opponents before it goes ahead and try to eat the food. Yet the probability of this situation happens is rare and we did minimize the maximum cost which is the Pacman's death.

Defence Choice
When the safe food is not available to an agent, itself will become a defensive agent and try to protect the food and chase down the opponent. When both agents do not have safe food available at the time, the choice will be made based on their scared time, distance to all remained foods. Furthermore, if the agent is scared, it will not even try to chase the opponent down. It will change back to offensive mode since I realize this by experiments it is wasting time and resources when the agent is in a scared state. It is better off for my agents to attack instead chase a target which may lose at the end since we assume that it is tough to chase down the enemy with only one agent even two in real life. Moreover, we are choosing the protecting position based on the number of ghosts seen and a method to calculate the best position to defend and a method to guess where the opponents are on our side. The best position method sums up all distance from current position to all remained food on our side and capsules with a ratio of 0.88 and 0.22 respectively, which means that capsules will yield a lower cost than foods. This will ensure that the position we choose is closer to the capsules since capsules are more important to regular foods. When Pacman notice the food change on our side, it will move toward that location which is the position the food just disappear. If the agent did not find the opponent within five steps, it would return to the best location. Meanwhile, the other agent tries to stop another agent from entering our side.

Choose the best defensive position and check the possible location of the enemy when food is been ate

Decision making
Since we decide to create only one agent to perform as two different behaviors, we must implement how to switch their mission. The very first line of each agent, we give them a boolean that better represent their tendency -- offensive or defensive. Our tendency of the whole program is very aggressive. When safe food is not available to both agents and the total remained foods is greater than five, both agents will set to the offensive mode that is actively seeking the opportunity to attack. All the decisions are made purely based on the concept of safe food which utilizes the Minimax theory to maximizing the minimum gain and minimizing the maximum loss. Since we are using only one agent class, based on the tendency we set at the beginning, we force to change the target for each agent by editing the distance with the target. Without changing the targets' distance, both Pacman will go for the same target which is useless.
